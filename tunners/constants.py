EVAL_STEPS = 500
LOGGING_STEPS = 500
LEARNING_RATE = 3e-5
PER_DEVICE_TRAIN_BATCH_SIZE = 16
PER_DEVICE_EVAL_BATCH_SIZE = 16
SAVE_TOTAL_LIMIT = 5
WARMUP_RATIO = 0.1
WEIGHT_DECAY = 0.01
NUM_TRAIN_EPOCH = 2

EARLY_STOPPING_THRESHOLD = 0.001
EARLY_STOPPING_PATIENCE = 10

MAX_MODEL_INPUTS_LENGHT = 128
MAX_LABELS_LENGTH = 16

# LLama model has different input formatting 
MAX_MODEL_INPUTS_LENGHT_LLAMA = 175
BOS_TOKEN = '<s>'
EOS_TOKEN = '</s>'
MAX_SEQUENCE_LENGTH = 50

HUGGING_FACE_TOKEN = ""
